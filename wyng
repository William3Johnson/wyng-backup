#!/usr/bin/env python3


###  Wyng â€“ Logical volume backup tool
###  Copyright Christopher Laprise 2018-2020 / tasket@protonmail.com
###  Licensed under GNU General Public License v3. See file 'LICENSE'.



#  editor width: 96  --------------------------------------------------------------------------
import sys, os, stat, shutil, subprocess, time, datetime
import re, mmap, bz2, zlib, gzip, tarfile, io, fcntl, tempfile
import argparse, configparser, hashlib, functools, uuid
import xml.etree.ElementTree    ; from itertools import islice
# For deduplication tests:
import ctypes, sqlite3, resource
from array import array


# ArchiveSet manages configuration and configured volume info

class ArchiveSet:
    def __init__(self, name, top, allvols=False):
        self.name        = name
        self.path        = pjoin(top,name)
        self.confpath    = pjoin(self.path,"archive.ini")
        self.hashindex   = {}
        self.vols        = {}
        self.allsessions = []
        self.in_process  = get_in_process(self.path)
        # persisted:
        self.chunksize   = bkchunksize
        self.compression = "zlib"
        self.compr_level = "4"
        self.hashtype    = "sha256"
        self.vgname      = None
        self.poolname    = None
        self.destsys     = None
        self.destdir     = "."
        self.destmountpoint = None
        self.uuid        = None

        self.conf = cp   = configparser.ConfigParser()
        cp.optionxform   = lambda option: option
        cp["var"]        = {}
        cp["volumes"]    = {}
        if not exists(self.confpath):
            self.uuid = str(uuid.uuid4())
            return
        cp.read(self.confpath)
        self.a_ints      = {"chunksize"}
        for name in cp["var"].keys():
            setattr(self, name, int(cp["var"][name]) \
                                if name in self.a_ints else cp["var"][name])

        # convert old delimiter to colon
        if self.destsys.startswith("qubes-ssh://") and self.destsys.find("|") > 0:
            self.destsys = self.destsys.replace("|",":",1)
            self.save_conf()

        for key in cp["volumes"]:
            if allvols or self.in_process or options.from_arch or \
            (cp["volumes"][key] != "disable" and \
            (len(options.volumes)==0 or key in options.volumes or options.dedup>1)):
                os.makedirs(pjoin(self.path,key), exist_ok=True)
                self.vols[key] = self.Volume(self, key, pjoin(self.path,key), self.vgname)
                self.vols[key].enabled = cp["volumes"][key] != "disable"
                self.allsessions += self.vols[key].sessions.values()

        # Create master session list sorted by date-time
        self.allsessions.sort(key=lambda x: x.localtime, reverse=True)


    def save_conf(self, fname=""):
        c = self.conf['var']
        c['chunksize']   = str(self.chunksize)
        c['compression'] = self.compression
        c['compr_level'] = self.compr_level
        c['hashtype']    = self.hashtype
        c['vgname']      = self.vgname
        c['poolname']    = self.poolname
        c['destsys']     = self.destsys
        c['destdir']     = self.destdir
        c['destmountpoint'] = self.destmountpoint
        c['uuid']        = self.uuid if self.uuid else str(uuid.uuid4())
        confdir  = os.path.dirname(self.confpath)   ; os.makedirs(confdir, exist_ok=True)
        confname = fname if fname else os.path.basename(self.confpath)
        with open(confdir+"/"+confname, "w") as f:
            self.conf.write(f)

    def add_volume(self, datavol):
        if datavol in self.conf["volumes"].keys():
            x_it(1, datavol+" is already configured.")

        if not self.Volume.volname_check(datavol):
            x_it(1, "Exiting.")

        if len(datavol) > 112:
            x_it(1, "Volume name must be 112 characters or less.")

        #self.vols[datavol] = self.Volume(datavol, pjoin(self.path,datavol),
        #                                 self.vgname)

        self.conf["volumes"][datavol] = "enable"
        self.save_conf()

    def delete_volume(self, datavol):
        if datavol in self.conf["volumes"].keys():
            del(self.conf["volumes"][datavol])
            self.save_conf()
        for ext in {".tick",".tock"}:
            if lv_exists(self.vgname, datavol+ext):  lv_remove(self.vgname, datavol+ext)
        get_lvm_vgs(self.vgname)

        if exists(pjoin(self.path,datavol)):
            shutil.rmtree(pjoin(self.path,datavol))


    class Volume:
        def __init__(self, archive, name, path, vgname):
            debug        = options.debug
            self.name    = name
            self.archive = archive
            self.path    = path
            self.vgname  = vgname
            self.present = lv_exists(vgname, name)
            self.enabled = False
            self.error   = False
            self.volsize = None
            self.mapfile = path+"/deltamap"
            self.mapped  = exists(self.mapfile)
            self.sessions= {}
            self.sesnames= []
            # persisted:
            self.format_ver = "0"
            self.uuid    = None
            self.first   = None
            self.last    = None
            self.que_meta_update = "false"

            # load volume info
            if exists(pjoin(path,"volinfo")):
                with open(pjoin(path,"volinfo"), "r") as f:
                    for ln in f:
                        vname, value = ln.strip().split(" = ")
                        setattr(self, vname, value)

            # load sessions in dict and ordered list of session names
            if debug:  print("\nVOLUME",self.name, self.first, self.last)
            sprev = self.last if self.last else "none"
            while sprev != "none":
                if debug:  print(sprev, end="  ")
                s = self.sessions[sprev] = self.Ses(self, sprev, path+"/"+sprev)
                sprev = s.previous    ; self.sesnames.insert(0, s.name)
                if sprev == "none" and self.first != s.name:
                    raise ValueError("PREVIOUS MISMATCH: %s/%s, EXPECTED %s" 
                                     % (self.name, s.name, self.first))
                if not s.present:
                    raise FileNotFoundError("ERROR: Manifest does not exist for "+name)

            if int(self.format_ver) > format_version:
                raise ValueError("Archive format ver = "+self.format_ver+
                                 ". Expected = "+format_version)
            #elif self.format_ver == "0":
            #    raise ValueError("No format_version; Alpha archive?")

            # use latest volsize
            self.volsize = self.sessions[self.last].volsize if self.sessions else 0

        def volname_check(vname):
            check = re.compile("^[a-zA-Z0-9\+\._-]+$")
            if check.match(vname) is None:
                print("Only characters A-Z 0-9 . + _ - are allowed in volume names.")
                return False
            if vname in (".",".."):
                print("Bad volume name.")
                return False
            return True

        def last_chunk_addr(self, vsize=None):
            if vsize is None:  vsize = self.volsize
            return (vsize-1) - ((vsize-1) % self.archive.chunksize)

        def map_exists(self):
            return exists(self.mapfile)

        #def map_used(self):
            ## fix: account for -tmp filename
            #return os.stat(self.mapfile).st_blocks

        # Based on last session size unless volume_size is specified.
        def mapsize(self, volume_size=None):
            if not volume_size:
                volume_size = self.volsize
            return (volume_size // self.archive.chunksize // 8) + 1

        def save_volinfo(self, fname="volinfo"):
            with open(pjoin(self.path,fname), "w") as f:
                print("format_ver =", format_version, file=f)
                print("uuid =", self.uuid if self.uuid else str(uuid.uuid4()), file=f)
                print("first =", self.first, file=f)
                print("last =", self.last, file=f)
                print("que_meta_update =", self.que_meta_update, file=f)

        def new_session(self, sname):
            ns = self.Ses(self, sname)
            ns.path = pjoin(self.path, sname)
            if self.first is None:
                ns.sequence = 0
                self.first = sname
            else:
                ns.previous = self.last
                ns.sequence = self.sessions[self.last].sequence + 1

            self.last = sname
            self.sesnames.append(sname)
            self.sessions[sname] = ns
            self.archive.allsessions.insert(0, ns)
            #os.makedirs(pjoin(self.path, sname))
            return ns

        def delete_session(self, sname, remove=True):
            ses = self.sessions[sname]
            index = self.sesnames.index(sname)
            indexall = self.archive.allsessions.index(ses)
            if len(self.sessions)==1 and not ses.saved:
                affected = "none"
                self.first = self.last = None
            elif sname == self.last:
                raise NotImplementedError("Cannot delete last session")
            else:
                affected = self.sesnames[index+1]
                self.sessions[affected].previous = ses.previous
                if index == 0:  self.first = self.sesnames[1]
            del self.sesnames[index]
            del self.sessions[sname]
            del self.archive.allsessions[indexall]

            if remove and exists(pjoin(self.path, sname)):
                shutil.rmtree(pjoin(self.path, sname))
            return affected


        class Ses:
            def __init__(self, volume, name, path=""):
                self.name     = name
                self.path     = path
                self.present  = exists(pjoin(path,"manifest"))
                self.saved    = False
                self.volume   = volume
                # persisted:
                self.uuid     = "none"
                self.localtime= None
                self.volsize  = None
                self.format   = None
                self.sequence = None
                self.previous = "none"
                attr_str      = ("localtime","format","previous","uuid")
                attr_int      = ("volsize","sequence")

                if path:
                    with open(pjoin(path,"info"), "r") as sf:
                        for ln in sf:
                            if ln.strip() == "uuid =":  continue
                            vname, value = ln.strip().split(" = ")
                            setattr(self, vname, 
                                int(value) if vname in attr_int else value)
                    if self.localtime is None or self.localtime == "None":
                        self.localtime = self.name[2:]

            def save_info(self, fname="info"):
                if not self.path:
                    raise ValueError("Path not set for save_info")
                self.volume.volsize = self.volsize
                self.saved = True
                with open(pjoin(self.path,fname), "w") as f:
                    print("uuid =",      self.uuid, file=f)
                    print("localtime =", self.localtime, file=f)
                    print("volsize =",   self.volsize, file=f)
                    print("format =",    self.format, file=f)
                    print("sequence =",  self.sequence, file=f)
                    print("previous =",  self.previous, file=f)


# Define absolute paths of commands

class CP:
    awk  = "/usr/bin/awk"    ; sed   = "/bin/sed"        ; sort     = "/usr/bin/sort"
    cat  = "/bin/cat"        ; mkdir = "/bin/mkdir"      ; python   = "/usr/bin/python3"
    mv   = "/bin/mv"         ; grep  = "/bin/grep"       ; ssh      = "/usr/bin/ssh"
    sh   = "/bin/sh"         ; tar   = "/bin/tar"        ; touch    = "/usr/bin/touch"
    rm   = "/bin/rm"         ; lvm   = "/sbin/lvm"       ; qvm_run  = "/usr/bin/qvm-run"
    tee  = "/usr/bin/tee"    ; sync  = "/bin/sync"       ; dmsetup  = "/sbin/dmsetup"
    find = "/usr/bin/find"   ; xargs = "/usr/bin/xargs"  ; sha256sum= "/usr/bin/sha256sum"
    cmp  = "/usr/bin/cmp"    ; gzip  = "/bin/gzip"
    mountpoint = "/bin/mountpoint"    ; thin_delta = "/usr/sbin/thin_delta"
    blkdiscard = "/sbin/blkdiscard"


class Lvm_VolGroup:
    def __init__(self, name):
        self.name = name
        self.lvs  = {}


class Lvm_Volume:
    colnames  = ["vg_name","lv_name","lv_attr","lv_size","lv_time","lv_uuid",
                 "pool_lv","thin_id","origin"]
    attr_ints = ["lv_size"]

    def __init__(self, members):
        for attr in self.colnames:
            val = members[self.colnames.index(attr)]
            setattr(self, attr, int(re.sub("[^0-9]", "", val)) if attr \
                in self.attr_ints else val)


# Retrieves survey of all LVs as vgs[].lvs[] dicts

def get_lvm_vgs(vg_to_lvols=None):
    global volgroups, l_vols

    do_exec([[CP.lvm, "lvs", "--units=b", "--noheadings", "--separator=::",
                "--options=" + ",".join(Lvm_Volume.colnames)]],
            out=tmpdir+"/volumes.lst")

    with open(tmpdir+"/volumes.lst", "r") as vlistf:
        for ln in vlistf:
            members = ln.strip().split("::")
            vgname = members[0] # Fix: use colname index
            lvname = members[1]
            if vgname not in volgroups.keys():
                volgroups[vgname] = Lvm_VolGroup(vgname)
            volgroups[vgname].lvs[lvname] = Lvm_Volume(members)

    if vg_to_lvols:  l_vols = volgroups[vg_to_lvols].lvs


def lv_exists(vgname, lvname):
    return vgname in volgroups.keys() \
            and lvname in volgroups[vgname].lvs.keys()


def vg_exists(vgname):
    try:
        do_exec([[CP.lvm, "vgdisplay", vgname]])
    except subprocess.CalledProcessError:
        return False
    else:
        return True


# Converts a non-cannonical LV path to LV name plus pool and vg names.
def get_lv_path_pool(path):
    try:
        p = subprocess.run([CP.lvm, "lvs", "--separator=::", "--noheadings",
                            "--options=lv_name,pool_lv,vg_name", path], check=True,
                            stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
    except subprocess.CalledProcessError:
        return "", "", ""
    else:
        return p.stdout.decode("utf-8").strip().split("::")


def lv_remove(vgname, lvname):
    do_exec([[CP.lvm,"lvchange", "-p", "rw", vgname+"/"+lvname]], check=False)
    do_exec([[CP.blkdiscard, pjoin("/dev",vgname,lvname)]], check=False)
    do_exec([[CP.lvm,"lvremove", "-f",vgname+"/"+lvname]])


# Reserve or release lvm thinpool metadata snapshot.
# arch must be a configured ArchiveSet; action must be "reserve" or "release".
def lvm_meta_snapshot(arch, action):
    checkerr = action == "reserve"
    do_exec([[CP.dmsetup,"message", arch.vgname+"-"+arch.poolname+"-tpool",
              "0", action+"_metadata_snap"]], check=checkerr)


# Initialize a new ArchiveSet:

def arch_init(aset):
    if not options.from_arch:
        if not options.local or not options.dest:
            x_it(1,"--local and --dest are required together.")
    elif options.dest:
        x_it(1,"--from and --dest cannot be used together.")
    if options.local:
        vgname, poolname = options.local.split("/")
        if not vg_exists(vgname):
            print("Warning: Volume group '%s' does not exist." % vgname)
        if not lv_exists(vgname, poolname):
            print("Warning: LV pool '%s' does not exist." % poolname)
        aset.vgname   = vgname
        aset.poolname = poolname

    dest    = options.from_arch if options.from_arch else options.dest
    destsys = delim = mountpoint = ""
    for i in url_types:
        if dest.startswith(i):
            destsys, delim, mountpoint = dest.replace(i,"",1).partition("/")
            break
    if (not mountpoint and not delim) or (i != "internal:" and not destsys) or \
       (i == "qubes-ssh://" and ("" in destsys.split(":") or len(destsys.split(":"))<2)):
        x_it(1,"Error: Malformed --dest specification.")

    aset.destsys        = i+destsys
    aset.destmountpoint = delim+mountpoint

    if options.subdir:
        if options.subdir.strip()[0] == "/":
            x_it(1,"Subdir cannot be absolute path.")
        aset.destdir    = options.subdir.strip()

    if options.from_arch:
        return

    if options.hashtype:
        if options.hashtype not in hash_funcs:
            x_it(1, "Hash function '"+options.hashtype+"' is not available on this system.")
        aset.hashtype = options.hashtype

    if options.compression:
        if ":" in options.compression:
            compression, compr_level = options.compression.strip().split(":")
        else:
            compression = options.compression.strip()
            compr_level = str(compressors[compression][1])
        compression = compression.strip()   ; compr_level = compr_level.strip()
        if compression not in compressors.keys() or not compr_level.isdecimal():
            x_it(1, "Invalid compression spec.")
        aset.compression = compression      ; aset.compr_level = compr_level

    if options.chfactor:
        # accepts an exponent from 1 to 6
        if not ( 0 < options.chfactor < 7 ):
            x_it(1, "Requested chunk size not supported.")
        aset.chunksize = (bkchunksize//2) * (2** options.chfactor)
        if aset.chunksize > 256 * 1024:
            print("Large chunk size set:", aset.chunksize)

    aset.save_conf()


# Get global configuration settings:

def get_configs():
    aname = "default"    ; bkdir = topdir + "/" + aname

    if options.from_arch:
        # Prepare a temporary metadata dir and init aset with it
        tmpmeta = tmpdir+"/var"    ; os.makedirs(tmpmeta+bkdir)
        aset    = ArchiveSet(aname, tmpmeta+topdir)
        arch_init(aset)
    else:
        aset    = ArchiveSet(aname, metadir+topdir)

    if aset.vgname:   lvm_meta_snapshot(aset, "release")

    if options.from_arch:
        return aset

    if options.action == "arch-init" and not aset.destmountpoint:
        arch_init(aset)
        x_it(0, "Done.")
    elif options.action == "arch-init":
        x_it(1, "Archive already initialized for "+aset.name)
    if not aset.destmountpoint:
        x_it(1,"Configuration not found.")

    return aset


def get_configs_remote():
    make_local = options.action == "arch-init"
    if not options.unattended:
        print("\nMetadata will be read from the archive;"
              "\nPlease check that the archive is in a trusted, secure condition!")
        ans = input("Proceed? [y/N] ")
        if ans.lower() not in ("y","yes"):
            x_it(0,"Stopped.")

    dest_run([destcd + bkdir +"  && "+CP.python+" "+tmpdir+"-rpc/dest_helper.py get-configs "
              +(options.volumes[0] if len(options.volumes)>0 else "")
             ], out=tmpdir+"/metadata.tgz")

    do_exec([[CP.tar,"-xzf",tmpdir+"/metadata.tgz"]], cwd=aset.path)
    if not exists(aset.path+"/archive.ini"):
        x_it(1,"Failed to retrieve remote configuration: No info.")

    if make_local:
        if exists(metadir+bkdir):
            os.replace(metadir+bkdir,
                       metadir+topdir+"/wyng.old/"+aset.name+"--"+time.strftime("%m%d-%H%M%S"))
        shutil.copytree(aset.path, metadir+bkdir)
        new_path = metadir+topdir
    else:
        new_path = os.path.dirname(aset.path)

    # Create final ArchiveSet object with imported config & current dest+local
    new_aset = ArchiveSet(aset.name, new_path)   ; new_aset.destsys = aset.destsys
    new_aset.destdir = aset.destdir              ; new_aset.destmountpoint= aset.destmountpoint
    if options.local:
        vgname, poolname = options.local.split("/")
        if not vg_exists(vgname):
            print("Warning: Volume group '%s' does not exist." % vgname)
        if not lv_exists(vgname, poolname):
            print("Warning: LV pool '%s' does not exist." % poolname)
        new_aset.vgname   = vgname    ; new_aset.poolname = poolname

    lvm_meta_snapshot(new_aset, "release")
    new_aset.save_conf()
    return new_aset


# Detect features of internal and destination environments:

def detect_internal_state():

    destsys = aset.destsys    ; desttype = None
    for dt in url_types:
        if destsys.find(dt) == 0:
            desttype = dt.rstrip(":/")    ; destsys = destsys[len(dt):]
            break
    if not desttype:
        raise ValueError("'%s' not an accepted type." % destsys)

    #    CP.thin_delta, CP.lvm, CP.blkdiscard,
    for prg in (CP.sort, CP.sed, CP.ssh if desttype=="ssh" else CP.sh):
        if not shutil.which(prg):
            raise RuntimeError("Required command not found: "+prg)

    try:
        p = subprocess.check_output([CP.thin_delta, "-V"])
    except:
        p = b""
    ver = p[:5].decode("UTF-8").strip()    ; target_ver = "0.7.4"
    if p and ver < target_ver:
        print("Note: Thin provisioning tools version", target_ver,
              "or later is recommended for stability. Installed version =", ver+".")


    #####>  Begin helper program  <#####

    dest_program = \
    '''#  Copyright Christopher Laprise 2018-2020
#  Licensed under GNU General Public License v3. See github.com/tasket/wyng-backup
import os, sys, shutil, subprocess, hashlib, gzip
cmd = sys.argv[1] ;    tmpdir = "''' + tmpdir + '''-rpc"
exists = os.path.exists   ; replace = os.replace;   remove = os.remove
if cmd != "merge" and exists(tmpdir+"/dest.lst.gz"):
    lstf = gzip.open(tmpdir+"/dest.lst.gz", "rt")
if cmd == "receive":
    for line in lstf:
        fname = line.strip()
        if not fname:  continue
        fsize = os.path.getsize(fname) if exists(fname) else 0
        i = sys.stdout.buffer.write(fsize.to_bytes(4,"big"))
        if fsize:
            with open(fname,"rb") as dataf:
                i = sys.stdout.buffer.write(dataf.read(fsize))
elif cmd == "merge":
    if not exists("../archive.ini") or not exists("volinfo"):
        print("Error: Not in volume dir.")   ; sys.exit(40)
    resume = "--resume" in sys.argv
    if resume:
        if exists("merge-init") or not exists("merge"):
            print("Merge: Init could not complete; Aborting merge.")   ; lstf.close
            if exists("merge-init"):
                for i in os.scandir("merge-init"):
                    if i.is_dir() and i.name.startswith("S_"):  os.replace(i.path, i.name)
            sys.exit(50)
    try:
        src_list = []   ; subdirs = set()
        lstf = gzip.open("merge.lst.gz", "rt")
        merge_target, target = lstf.readline().split()
        while True:
            ln = lstf.readline().strip()
            if ln == "###":  break
            src_list.append(ln)
        if not resume:
            print("Merge: Initialization.")
            for f in (target+"/info", target+"/manifest", "volinfo"):
                if not exists(f+".tmp"):  raise FileNotFoundError(f)
            for ex in ("","-init"):  shutil.rmtree("merge"+ex, ignore_errors=True)
            os.makedirs("merge-init")   ; replace(merge_target, "merge-init/"+merge_target)
            for src in src_list:   replace(src, "merge-init/"+src)
            replace("merge-init", "merge")
    except Exception as err:
        if exists("merge-init"):
            for i in os.scandir("merge-init"):
                if i.is_dir() and i.name.startswith("S_"):  os.replace(i.path, i.name)
        print(err)    ; sys.exit(50)
    try:
        print("Merge: remove/replace files.")   ; open("CHECK-mv-rm","w").close()
        # CD
        os.chdir("merge")
        for src in src_list:  # Enh: replace os.scandir w manifest method
            for i in os.scandir(src):
                if i.is_dir():   subdirs.add(i.name)
        for sdir in subdirs:
            os.makedirs(merge_target+"/"+sdir, exist_ok=True)
        for line in lstf:
            ln = line.split() # default split() does strip()
            if ln[0] == "rename" and (not resume or exists(ln[1])):
                replace(ln[1], ln[2])
            elif ln[0] == "-rm" and exists(ln[1]):
                remove(ln[1])
        open("CHECK-END-mv-rm","w").close()   ; lstf.close()
    except Exception as err:
        print(err)    ; sys.exit(60)
    try:
        print("Merge: Finalize tmp.")   ; open("CHECK-finalize-tmp","w").close()
        for f in ("/info", "/manifest"):
            if not resume or exists(target+f+".tmp"):  replace(target+f+".tmp", merge_target+f)
        if not resume or exists("../volinfo.tmp"):  replace("../volinfo.tmp", "../volinfo")
    except Exception as err:
        print(err)    ; sys.exit(70)
    try:
        print("Merge: Finalize target dir.")   ; open("CHECK-finalize-target","w").close()
        # CD
        os.chdir("..")   ; replace("merge/"+merge_target, target)   ; remove("../in_process")
    except Exception as err:
        print(err)    ; sys.exit(80)
    open("merge/CHECK-cleanup","w").close()   ; shutil.rmtree("merge")   ; remove(lstf.name)
elif cmd == "dedup":
    ddcount = 0
    for line in lstf:
        source, dest = line.split()
        if os.stat(source).st_ino != os.stat(dest).st_ino:
            os.link(source, dest+"-lnk")
            replace(dest+"-lnk", dest)
            ddcount += 1
    print(ddcount, "reduced.")
elif cmd == "get-configs":
    with open(tmpdir+"/tar.lst","w") as tarlstf:
        vols = [sys.argv[2]] if len(sys.argv)>2 else \
                [x.name for x in os.scandir() if x.is_dir()]
        for volname in vols:
            print(volname+"/volinfo", file=tarlstf)
            for ses in os.scandir(volname):
                if ses.is_dir() and ses.name.rfind("-tmp") == -1:
                    print(volname+"/"+ses.name+"/info", file=tarlstf)
                    print(volname+"/"+ses.name+"/manifest", file=tarlstf)
        if exists("in_process"):  print("in_process", file=tarlstf)
    p = subprocess.check_call(["tar","-czf","-","--verbatim-files-from",
    "--files-from="+tmpdir+"/tar.lst","archive.ini"], stderr=subprocess.DEVNULL)
    '''
    with open(tmpdir +"/rpc/dest_helper.py", "wb") as progf:
        progf.write(bytes(dest_program, encoding="UTF=8"))

    #####>  End helper program  <#####

    return destsys, desttype


def detect_dest_state(destsys):
    global dest_run_map
    dest_run_map     = {"internal": [CP.sh],
                        "ssh":      [CP.ssh] + ssh_opts + [destsys],
                        "qubes":    [CP.qvm_run, "-p", destsys],
                        "qubes-ssh":[CP.qvm_run, "-p", destsys.split(":")[0]]
                        }

    if (options.action not in local_actions and destsys is not None) \
        or options.remap or options.from_arch:

        if desttype == "qubes-ssh":
            # fix: possibly remove dargs and use dest_run()
            dargs = dest_run_map["qubes"][:-1] + [destsys.split(":")[0]]

            cmd = dargs + [shell_prefix \
                  +CP.rm+" -rf "+tmpdir+"-rpc  &&  mkdir -p "+tmpdir+"-rpc"
                  ]
            do_exec([cmd])

        # Fix: get OSTYPE env variable
        online = True
        try:
            cmd = ["mountpoint -q '"+aset.destmountpoint
                    +"' && mkdir -p '"+aset.destmountpoint+"/"+aset.destdir+topdir
                    +"' && cd '"+aset.destmountpoint+"/"+aset.destdir+topdir+"'"

                    # send helper program to remote dest
                    +((" && rm -rf "+tmpdir+"-rpc  &&  mkdir -p "+tmpdir+"-rpc"
                      +" && cat >"+tmpdir+"-rpc/dest_helper.py"))

                    # check in_process status on remote
                    +"  && { if [ -e "+aset.name+"/in_process ]; then"
                    +"  echo 'in_process yes'; fi }"

                    +"  && touch archive.dat"
                    +(" && ln -f archive.dat .hardlink" if options.dedup > 1 else "")
                    ]
            dest_run(cmd, out=tmpdir+"/dest-state-log",
                     infile=tmpdir+"/rpc/dest_helper.py")

        except subprocess.CalledProcessError:
            online = False

    else:
        online = False

    dest_in_proc = online and do_exec([[CP.grep, "in_process yes", tmpdir+"/dest-state-log"]],
                                      check=False) == 0
    return online, dest_in_proc


# Set or clear state for the archive as 'in_process' in case of interruption during write.
# Format is list containing strings or list of strings. For latter, '/' is the delimiter.

def set_in_process(outer_list, tmp=False, dest=True):

    if outer_list is None:
        if dest:  dest_run([destcd + bkdir + " && rm -f in_process"])
        for ext in ("", ".tmp", "_retry"):
            if exists(aset.path+"/in_process"+ext):  os.remove(aset.path+"/in_process"+ext)
        aset.in_process = None
        return

    with open(aset.path+"/in_process.tmp", "w") as ipf:
        for ln in outer_list:
            print(ln if type(ln) is str else "/".join(ln), file=ipf)

    if dest:
        dest_run([destcd + bkdir +" && cat >in_process.tmp"
                                +(" && mv in_process.tmp in_process") if not tmp else ""],
                infile=aset.path+"/in_process.tmp")
    if not tmp:
        os.rename(aset.path+"/in_process.tmp", aset.path+"/in_process")
    aset.in_process = outer_list


def get_in_process(archpath=""):
    outer_list = []
    if not archpath: archpath = aset.path
    if exists(archpath+"/in_process"):
        with open(archpath+"/in_process", "r") as ipf:
            for ln in ipf:
                if ln.find("/") >= 0:
                    outer_list.append(ln.strip().split("/"))
                else:
                    outer_list.append(ln.strip())

            #self.in_process = [x.strip() for x in ipf]
    return outer_list if len(outer_list) else None


# Run system commands with pipes, without shell:
# 'commands' is a list of lists, each element a command line.
# If multiple command lines, then they are piped together.
# 'out' redirects the last command output to a file; append mode can be
# selected by beginning 'out' path with '>>'.
# List of commands may include 'None' instead of a child list; these will be ignored.

def do_exec(commands, cwd=None, check=True, out="", infile="", text=False):
    ftype   = "t" if text else "b"
    outmode = "a" if out.startswith(">>") else "w"
    out = out.lstrip(">>")
    if cwd and out and out[0] != "/":
        out = pjoin(cwd,out)
    if cwd and infile and infile[0] != "/":
        infile = pjoin(cwd,infile)
    outfunc = gzip.open if out.endswith(".gz") else open
    infunc  = gzip.open if infile.endswith(".gz") else open
    outf = outfunc(out, outmode+ftype) if out else subprocess.DEVNULL
    inf  = infunc(infile, "r"+ftype) if infile else subprocess.DEVNULL
    errf = open(tmpdir+"/err.log", "a")  ; print("--+--", file=errf)
    commands = [x for x in commands if x is not None]

    # Start each command, linking them via pipes
    procs = []
    for i, clist in enumerate(commands):
        p = subprocess.Popen(clist, cwd=cwd, stdin=inf if i==0 else procs[i-1].stdout,
                             stdout=outf if i==len(commands)-1 else subprocess.PIPE,
                             stderr=errf)
        if len(procs):  procs[-1].stdout.close()
        procs.append(p)

    # Monitor and control processes
    while True:
        err = None    ; finish = timeout = False
        for p1 in reversed(procs):
            retcode = p1.poll()
            if not finish and retcode is None:
                try:
                    p1.wait(timeout=2)
                except subprocess.TimeoutExpired:
                    timeout = True
                    continue
                retcode = p1.returncode   ; finish = True
                if check and (retcode > 0):
                    err = p1              ; finish = True
            elif finish and retcode is None:
                p1.terminate()
                continue

        if err or not timeout:
            break

    for f in [inf, outf, errf]:
        if type(f) is not int: f.close()
    if err and check:
        raise subprocess.CalledProcessError(err.returncode, err.args)

    return procs[-1].returncode


# Run system commands on destination

def dest_run(commands, dest_type=None, infile="", out="", check=True):
    if dest_type is None:
        dest_type = desttype

    cmd = dest_run_args(dest_type, commands)
    return do_exec([cmd], infile=infile, out=out, check=check)


# Build command lists that can be shunted to remote systems.
# The input commands are stored in a temp file and a standard command that
# runs the temp file is returned.

def dest_run_args(dest_type, commands):

    # shunt commands to tmp file
    with tempfile.NamedTemporaryFile(dir=tmpdir, delete=False) as tmpf:
        cmd = bytes(shell_prefix + " ".join(commands) + "\n", encoding="UTF-8")
        tmpf.write(cmd)
        remotetmp = os.path.basename(tmpf.name)

    if dest_type in {"qubes","qubes-ssh"}:
        do_exec([[CP.qvm_run, "-p",
                  (destsys if dest_type == "qubes" else destsys.split(":")[0]),
                  CP.mkdir+" -p "+tmpdir+"-rpc"
                  +" && "+CP.cat+" >"+pjoin(tmpdir+"-rpc",remotetmp)
                ]], infile=pjoin(tmpdir,remotetmp))
        if dest_type == "qubes":
            add_cmd = [CP.sh+" "+pjoin(tmpdir+"-rpc",remotetmp)]
        elif dest_type == "qubes-ssh":
            add_cmd = [CP.ssh+" "+" ".join(ssh_opts)+" "+destsys.split(":")[1]
                       +' "$('+CP.cat+' '+pjoin(tmpdir+"-rpc",remotetmp)+')"']

    elif dest_type == "ssh":
        #add_cmd = [' "$(cat '+pjoin(tmpdir,remotetmp)+')"']
        add_cmd = [cmd]

    elif dest_type == "internal":
        add_cmd = [pjoin(tmpdir,remotetmp)]

    ret = dest_run_map[dest_type] + add_cmd
    #print("CMD",ret)
    return ret


# Prepare snapshots and check consistency with metadata.
# Must run get_lvm_vgs() again after this.

def prepare_snapshots(datavols):

    ''' Normal precondition will have a snap1vol already in existence in addition
    to the local datavol. Here we create a fresh snap2vol so we can compare
    it to the older snap1vol. Then, depending on monitor or backup mode, we'll
    accumulate delta info and possibly use snap2vol as source for a
    backup session.
    '''

    print("Preparing snapshots...")
    dvs, nvs = [], []                   ; vgname = aset.vgname
    for datavol in datavols:
        # 'mapfile' is the deltamap file, 'snapXvol' are the .tick and .tock snapshots.
        # .tick holds vol state between send/monitor ops;
        # .tock is new snapshot which is compared w .tick and then replaces it (send/monitor).
        vol      = aset.vols[datavol]   ; mapfile  = vol.mapfile
        snap1vol = datavol + ".tick"    ; snap2vol = datavol + ".tock"


        if not lv_exists(vgname, datavol):
            print("Warning:", datavol, "does not exist!")
            continue

        # Remove stale snap2vol
        if lv_exists(vgname, snap2vol):  lv_remove(vgname, snap2vol)

        # Reclaim deltamap from interrupted session.
        # Future: Expand recovery to start send-resume
        if exists(mapfile+"-tmp"):  os.rename(mapfile+"-tmp", mapfile)

        # Make deltamap or initial snapshot if necessary:
        if len(vol.sessions):
            if lv_exists(vgname, snap1vol) and not exists(mapfile) \
                    and vol.sessions[vol.last].uuid == l_vols[snap1vol].lv_uuid:
                # Latest session matches current snapshot; OK to make blank map.
                init_deltamap(mapfile, vol.mapsize())

        elif not monitor_only:
            # This is new vol and we're send-ing, so make initial .tick snapshot.
            if not lv_exists(vgname, snap1vol):
                do_exec([[CP.lvm, "lvcreate", "-pr", "-kn",
                    "-ay", "-s", vgname+"/"+datavol, "-n", snap1vol]])
                volgroups[vgname].lvs[snap1vol] = "placeholder"
                print("  Initial snapshot created for", datavol)
            # add to 'new' vols list
            nvs.append(datavol)

        if len(vol.sessions) and (not exists(mapfile) or not lv_exists(vgname, snap1vol)):
            # Handle unusual circumstance where remapping is needed. Vol has history
            # but .tick and/or deltamap are still missing after above checks.
            # In this case (which can occur after importing w '--from') the vol must
            # be scanned to determine if any differences exist with lastest backup session.
            if options.remap:
                print("Snapshot and deltamap not paired for "+datavol+"; Remapping...")
                receive_volume(datavol, save_path="", diff=True)
            else:
                x_it(1,"Error: Deltamap not paired to "+datavol
                     +" snapshot. To resolve, run wyng with the '--remap' option.")

        # Make fresh snap2vol
        do_exec([[CP.lvm, "lvcreate", "-pr", "-kn", "-ay",
            "-s", vgname+"/"+datavol, "-n",snap2vol]])

        # Volume is OK, add to list of vols.
        if datavol not in nvs:  dvs.append(datavol)

    return dvs, nvs


# Get raw lvm deltas between snapshots
# Runs the 'thin_delta' tool to output diffs between vol's old and new snapshots.
# Result can be read as an xml file by update_delta_digest().

def get_lvm_deltas(datavols):
    vgname   = aset.vgname    ; poolname = aset.poolname
    print("Acquiring deltas.")

    # Reserve a metadata snapshot for the LVM thin pool; required for a live pool.
    lvm_meta_snapshot(aset, "reserve")

    err = False
    try:
        for datavol in datavols:
            snap1vol = datavol + ".tick"    ; snap2vol = datavol + ".tock"
            do_exec([[CP.thin_delta, "-m",  "--thin1=" + l_vols[snap1vol].thin_id,
                                            "--thin2=" + l_vols[snap2vol].thin_id,
                                    "/dev/mapper/"+vgname+"-"+poolname+"_tmeta"],
                        [CP.grep, "-v", "<same .*\/>$"]
                    ],  out=tmpdir+"/delta."+datavol )
    except:
        err = True
    finally:
        lvm_meta_snapshot(aset, "release")

    if err:
        x_it(1, "ERROR running thin_delta process.")


# update_delta_digest: Translates raw lvm delta information
# into a bitmap (actually chunk map) that repeatedly accumulates change status
# for volume block ranges until a 'send' command is successfully completed and
# the mapfile is cleared.

def update_delta_digest(datavol):

    if monitor_only:  print("Scanning for changes.")
    vol         = aset.vols[datavol]      ; chunksize   = aset.chunksize
    if len(vol.sessions) == 0:  return False # nothing to compare
    snap1vol    = vol.name + ".tick"      ; snap1size   = l_vols[snap1vol].lv_size
    snap2vol    = vol.name + ".tock"      ; snap2size   = l_vols[snap2vol].lv_size

    # Put deltamap into a temporary state
    os.rename(vol.mapfile, vol.mapfile+"-tmp")

    # Get xml parser and initialize vars
    dtree       = xml.etree.ElementTree.parse(tmpdir+"/delta."+datavol).getroot()
    dblocksize  = int(dtree.get("data_block_size"))
    bmap_byte   = lastindex   = dnewblocks   = dfreedblocks = 0

    # Check for volume size increase;
    # Chunks from 'markall_pos' onward will be marked for backup.
    next_chunk_addr  = vol.last_chunk_addr() + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile+"-tmp", "r+b") as bmapf:
        snap_ceiling = max(snap1size, snap2size) // bs    ; chunkblocks = chunksize // bs
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)

        # Cycle through the 'thin_delta' metadata, marking bits in bmap_mm as needed.
        # Entries carry a block position 'blockbegin' and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for delta in dtree.find("diff"):
            blockbegin = int(delta.get("begin")) * dblocksize
            if blockbegin >= snap_ceiling:  continue
            blocklen   = int(delta.get("length")) * dblocksize
            blockend   = min(blockbegin+blocklen, snap_ceiling)
            if delta.tag in ("different", "right_only"):
                dnewblocks += blockend - blockbegin
            elif delta.tag == "left_only":
                dfreedblocks += blockend - blockbegin
            else: # superfluous tag
                continue

            # 'blockpos' iterates over disk blocks, with thin LVM constant of 512 bytes/block.
            # dblocksize (local) & chunksize (dest) may be somewhat independant of each other.
            # bmap_byte & lastindex buffer the changes, but perf impact not checked.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8
                if bmap_pos != lastindex: #save it
                    bmap_mm[lastindex] |= bmap_byte      ; bmap_byte = 0
                bmap_byte |= 1 << (volsegment % 8)   ; lastindex = bmap_pos

        if bmap_byte:  bmap_mm[lastindex] |= bmap_byte # save last

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewblocks += (bmap_size - markall_pos) * 8 * dblocksize

    if monitor_only and dnewblocks+dfreedblocks > 0:
        print(" ", dnewblocks, "changes,", dfreedblocks, "discards.")

    return dnewblocks+dfreedblocks > 0


# Send volume to destination.
#
# send_volume() has two main modes which are full (send_all) and incremental. After send
# finishes a full session, the volume will have a blank deltamap and .tick snapshot to
# track changes. After an incremental send, snapshots are rotated and the deltamap is reset.
#
# The 'dedup' deduplication level also represents a range of modes, where '0' means that all
# marked chunks will be sent unconditionally, '1' means a chunk won't be sent if its marked
# but same content, and '>= 2' will check w the archive-wide index and link it if matched.

def send_volume(datavol, localtime):

    vol         = aset.vols[datavol]            ; prior_size  = vol.volsize 
    snap2vol    = vol.name + ".tock"            ; snap2size   = l_vols[snap2vol].lv_size
    bmap_size   = vol.mapsize(snap2size)        ; chunksize   = aset.chunksize
    chdigits    = max_address.bit_length()//4   ; chformat    = "%0"+str(chdigits)+"x"
    bksession   = "S_"+localtime                ; sdir        = pjoin(datavol, bksession)
    send_all    = len(vol.sessions) == 0        ; dedup       = options.dedup

    if dedup and not send_all:
        # Our chunks are usually smaller than LVM's, so generate a full manifest to detect
        # significant amount of unchanged chunks that are flagged in the delta bmap.
        fullmanifest = open(merge_manifests(datavol), "r")
        fullmanifest_readline = fullmanifest.readline
    else:
        fullmanifest = None

    ses = vol.new_session(bksession)
    ses.localtime = localtime
    ses.uuid      = l_vols[snap2vol].lv_uuid
    ses.volsize   = snap2size
    ses.format    = "folders"
    ses.path      = vol.path+"/"+bksession+"-tmp"

    # testing deduplication types:
    # Code from init_dedup_indexN() localized here for efficiency.
    dedup_idx     = dedup_db = None
    fman_hash     = fman_fname = ""
    if dedup == 3: # sql
        dedup_db  = aset.hashindex
        cursor    = dedup_db.cursor()
        c_uint64  = ctypes.c_uint64
        c_int64   = ctypes.c_int64
        ddsessions= aset.allsessions
    elif dedup == 4: # array tree
        ddsessions, hashtree, ht_ksize, hashdigits, hash_w, hash0len, \
        dataf, chtree, chdigits, ch_w, ses_w \
                  = aset.hashindex
        ht_ksize  = ht_ksize//2
        hsegs     = hash_w//hash0len
        chtree_max= 2**(chtree[0].itemsize*8)
        idxcount  = dataf.tell() // (ch_w+ses_w)
    elif dedup == 5: # bytearray tree
        ddsessions, hashtree, ht_ksize, hashdigits, hash_w, \
        dataf, chtree, chdigits, ch_w, ses_w \
                  = aset.hashindex
        chtree_max= 2**(chtree[0].itemsize*8)
        ht_ksize  = ht_ksize//2
        idxcount  = dataf.tell() // (ch_w+ses_w)
    if dedup > 1:
        ddsessions.append(ses)    ; ses_index = ddsessions.index(ses)

    # Set current dir and make new session folder
    os.chdir(metadir+bkdir)
    os.makedirs(sdir+"-tmp")

    zeros     = bytes(chunksize)
    bcount    = ddbytes = 0
    addrsplit = -address_split[1]
    lchunk_addr = vol.last_chunk_addr(snap2size)

    use_zlib = aset.compression == "zlib" # workaround for old zlib limitation
    if aset.compression == "bz2":
        compress = functools.partial(bz2.compress, compresslevel=int(aset.compr_level))
    elif aset.compression == "zlib":
        compress_zlib = zlib.compress    ; compresslevel = int(aset.compr_level)

    # Use tar to stream files to destination
    stream_started = False
    untar_cmd = [destcd + " && mkdir -p ."+bkdir+"/"+sdir+"-tmp"
                        + " && "+destcd + bkdir
                        + " && rm -f .set  &&  tar -xmf - && sync -f "+datavol]

    # Open source volume and its delta bitmap as r, session manifest as w.
    with open(pjoin("/dev",aset.vgname,snap2vol),"rb") as vf, \
         open(sdir+"-tmp/manifest", "w") as hashf,             \
         open("/dev/zero" if send_all else vol.mapfile+"-tmp","rb") as bmapf:

        vf_seek = vf.seek; vf_read = vf.read
        gethash = hash_funcs[aset.hashtype]   ; BytesIO = io.BytesIO

        # Show progress in increments determined by 1000/checkpt_pct
        # where '200' results in five updates i.e. in unattended mode.
        checkpt = checkpt_pct = 335 if options.unattended else 1
        addr    = percent = 0

        # Feed delta bmap to inner loop in pieces segmented by large zero delimeter.
        # This allows skipping most areas when changes are few.
        zdelim  = bytes(64)    ; bmseg = minibmap = None    ; bmap_list = []

        while addr < snap2size:
            if len(bmap_list):
                # At boundary inside list, so use islice to jump ahead here.
                if fullmanifest:  list(islice(fullmanifest, len(zdelim)*8))
                addr += chunksize*len(zdelim)*8
                minibmap = bmap_list.pop()
            elif not send_all:
                # Empty; Get more.
                bmseg = bmapf.read(25000)
                if bmseg:
                    # split(zdelim) shows us where large unmodified zones exist in bmap.
                    # Resulting list is reversed so we can use pop().
                    bmap_list.extend(bmseg.split(zdelim))    ; bmap_list.reverse()
                else:
                    break
                minibmap = bmap_list.pop()

            # Cycle over range of chunk addresses.
            for chunk, addr in enumerate(range( addr, snap2size if send_all
                        else min(snap2size, addr+len(minibmap)*8*chunksize), chunksize)):

                destfile = "x"+chformat % addr

                if fullmanifest: # level 1
                    fman_hash, fman_fname = fullmanifest_readline().split()
                    if fman_fname != destfile:
                        raise ValueError("expected manifest addr %s, got %s"
                                         % (destfile, fman_fname))
                else:
                    fman_hash = ""

                # Skip chunk if its deltamap bit is off.
                if not send_all and not (minibmap[chunk//8] & (1 << chunk%8)):  continue

                vf_seek(addr)    ; buf = vf_read(chunksize)

                # Show progress.
                percent = int(addr/snap2size*1000)
                if percent >= checkpt:
                    print("  %.1f%%   %dMB " % (percent/10, bcount//1000000),
                            end="\x0d", flush=True)
                    checkpt += checkpt_pct

                # Compress & write only non-empty and last chunks
                if buf == zeros and addr < lchunk_addr:
                    if fman_hash != "0":  print("0", destfile, file=hashf)
                    continue

                buf    = compress_zlib(buf, compresslevel) if use_zlib else compress(buf)
                bhash  = gethash(buf)   ; hexhash = bhash.hexdigest()

                # level 1 dedup
                if fman_hash == hexhash:  continue

                # Start tar stream
                if not stream_started:
                    untar = subprocess.Popen(dest_run_args(desttype, untar_cmd),
                            stdin =subprocess.PIPE,    stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL)
                    tarf = tarfile.open(mode="w|", fileobj=untar.stdin)
                    tarf_addfile = tarf.addfile; TarInfo = tarfile.TarInfo
                    LNKTYPE = tarfile.LNKTYPE
                    stream_started = True

                # Add buffer to stream
                tar_info = TarInfo("%s-tmp/%s/%s" % 
                                (sdir, destfile[1:addrsplit], destfile))
                print(hexhash, destfile, file=hashf)

                # If chunk already in archive, link to it
                if dedup < 2:
                    pass

                elif dedup == 3:
                    bhashb = bhash.digest()
                    row    = cursor.execute("SELECT chunk,ses_id FROM hashindex "
                            "WHERE id = ?", (bhashb,)).fetchone()
                    if row:
                        ddch, ddses_i = row
                        ddses = ddsessions[ddses_i]
                        ddchx = chformat % (c_uint64(ddch).value)
                        tar_info.type = LNKTYPE
                    else:
                        # perf fix: use execute_many + index of waiting inserts
                        cursor.execute("INSERT INTO hashindex(id,chunk,ses_id)"
                            " VALUES(?,?,?)", 
                            (bhashb, c_int64(addr).value, ses_index))

                elif dedup == 4:
                    bhashb = bhash.digest()
                    i      = int.from_bytes(bhashb[:ht_ksize], "big")
                    ht     = hashtree[i]; ct = chtree[i]
                    while True:
                        try:
                            pos = ht.index(int.from_bytes(bhashb[:hash0len],
                                                        "little"))
                        except ValueError:
                            if idxcount < chtree_max:
                                hashtree[i].frombytes(bhashb)
                                chtree[i].append(idxcount)
                                dataf.write(ses_index.to_bytes(ses_w,"big"))
                                dataf.write(addr.to_bytes(ch_w,"big"))
                                idxcount += 1
                                break # while

                        if pos % hsegs == 0 and \
                            ht[pos+1:pos+hsegs].tobytes() == bhashb[hash0len:]:
                            # First hash segment matched; test remaining segments.
                            data_i = ct[pos//hsegs]
                            dataf.seek(data_i*(ses_w+ch_w))
                            ddses  = ddsessions[int.from_bytes(
                                        dataf.read(ses_w),"big")]
                            ddchx  = dataf.read(ch_w).hex().zfill(chdigits)
                            dataf.seek(0,2)
                            tar_info.type = LNKTYPE
                            break # while

                        pos += hsegs - (pos % hsegs)
                        ht = ht[pos:]; ct = ct[pos//hsegs:]

                elif dedup == 5:
                    bhashb = bhash.digest()
                    i      = int.from_bytes(bhashb[:ht_ksize], "big")

                    pos = hashtree[i].find(bhashb)
                    if pos % hash_w == 0:
                        data_i = chtree[i][pos//hash_w]
                        dataf.seek(data_i*(ses_w+ch_w))
                        ddses = ddsessions[int.from_bytes(
                                dataf.read(ses_w),"big")]
                        ddchx = dataf.read(ch_w).hex().zfill(chdigits)
                        dataf.seek(0,2)
                        tar_info.type = LNKTYPE
                    elif idxcount < chtree_max:
                        hashtree[i].extend(bhashb)
                        chtree[i].append(idxcount)
                        dataf.write(ses_index.to_bytes(ses_w,"big"))
                        dataf.write(addr.to_bytes(ch_w,"big"))
                        idxcount += 1

                if tar_info.type == LNKTYPE:
                    tar_info.linkname = "%s/%s/%s/x%s" % \
                        (ddses.volume.name,
                            ddses.name+"-tmp" if ddses is ses else ddses.name,
                            ddchx[:addrsplit],
                            ddchx)
                    ddbytes += len(buf)
                    tarf_addfile(tarinfo=tar_info)
                else:
                    tar_info.size = len(buf)
                    tarf_addfile(tarinfo=tar_info, fileobj=BytesIO(buf))
                    bcount += len(buf)

            if minibmap or send_all:  addr += chunksize

    # Send session info, end stream and cleanup
    if stream_started:
        print("  100%  ", ("%.1f" % (bcount/1000000)) +"MB",
              ("  ( reduced %0.1fMB )" % (ddbytes/1000000)) 
              if ddbytes else "")

        # Save session info
        ses.save_info()
        for session in vol.sessions.values() \
                        if vol.que_meta_update == "true" else [ses]:
            tarf.add(pjoin(vol.name, os.path.basename(session.path)))
        vol.que_meta_update = "false"
        vol.save_volinfo("volinfo-tmp")
        tarf.add(datavol+"/volinfo-tmp")
        tarf.add(os.path.basename(aset.confpath))

        #print("Ending tar process ", end="")
        tarf.close()
        untar.stdin.close()
        try:
            untar.wait(timeout=60)
        except subprocess.TimeoutExpired:
            print("Warning: tar process timeout.")
            retcode = 15
            untar.terminate()
        else:
            retcode = untar.poll()

        if retcode < 0:
            raise RuntimeError("tar transport failure code %d" % retcode)

        # Finalize on VM/remote
        dest_run([ destcd + bkdir
            +" && touch .set"
            +" && mv "+sdir+"-tmp "+sdir
            +" && mv "+datavol+"/volinfo-tmp "+datavol+"/volinfo"
            +" && sync -f "+datavol+"/volinfo"])
        # Local finalize
        os.replace(ses.path, ses.path[:-4])    ; ses.path = ses.path[:-4]
        os.replace(vol.path+"/volinfo-tmp", vol.path+"/volinfo")

    else:
        vol.delete_session(bksession)
        shutil.rmtree(aset.path+"/"+sdir+"-tmp")

    if bcount == 0:
        print("  No changes.")

    if dedup and options.debug:
        show_mem_stats()

    return stream_started


# Build deduplication hash index and list

def init_dedup_index1(listfile=""):
    return


def init_dedup_index3(listfile=""):

    addrsplit = -address_split[1]
    sessions  = aset.allsessions
    c_int64   = ctypes.c_int64
    chdigits  = max_address.bit_length() // 4
    chformat  = "%0"+str(chdigits)+"x"
    ctime     = time.time()

    db     = sqlite3.connect(tmpdir+"/hashindex.db")
    #db    = sqlite3.connect(":memory:")
    cursor = db.cursor()
    cursor.execute('''
        CREATE TABLE hashindex(id BLOB PRIMARY KEY ON CONFLICT IGNORE,
        chunk INTEGER, ses_id INTEGER
        )''')
    insert_phrase = 'INSERT INTO hashindex(id, chunk, ses_id) VALUES(?,?,?)'
    cursor.execute('PRAGMA cache_size = 15000')
    #cursor.execute('PRAGMA synchronous = OFF')
    #cursor.execute('PRAGMA journal_mode = OFF')

    if listfile:
        dedupf = open(tmpdir+"/"+listfile, "w")

    inserts = []; rows = 0
    for sesnum, ses in enumerate(sessions):
        volname = ses.volume.name; sesname = ses.name
        with open(pjoin(ses.path,"manifest"),"r") as manf:
            for ln in manf:
                line = ln.split()
                if line[0] == "0":
                    continue
                bhash = bytes().fromhex(line[0])
                uint  = int(line[1][1:],16)
                addr  = c_int64(uint)

                inserts.append((bhash, addr.value, sesnum))
                # Insert only 1 at a time when generating a listfile.
                if listfile or not len(inserts) % 2000:
                    cursor.executemany(insert_phrase, inserts)
                    rows += cursor.rowcount
                    inserts.clear()

                    if listfile and cursor.rowcount < 1:
                        row = cursor.execute("SELECT chunk,ses_id FROM hashindex "
                                "WHERE id = ?", (bhash,)).fetchone()
                        if row:
                            ddch, ddses_i = row
                            ddses = sessions[ddses_i]
                            ddchx = chformat % ddch
                            print("%s/%s/%s/x%s %s/%s/%s/%s" % \
                                (ddses.volume.name, ddses.name, ddchx[:addrsplit], ddchx,
                                volname, sesname, line[1][1:addrsplit], line[1]),
                                file=dedupf)

    if len(inserts):
        cursor.executemany(insert_phrase, inserts)
        rows += cursor.rowcount
    db.commit()
    aset.hashindex = db

    if listfile:
        dedupf.close()

    ####
    if not options.debug:  return
    print("\nIndexed in %.1f seconds." % int(time.time()-ctime))
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\nMemory use: Max %dMB, index count: %d" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024,
        rows)
        )
    print("Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


def init_dedup_index4(listfile=""):

    ctime     = time.time()
    # Define arrays and element widths
    hashdigits = hash_bits // 4 # 4bits per hex digit
    hash_w     = hash_bits // 8
    hash0len   = 8        # "Q" ulonglong = 8bytes
    hsegs      = hash_w//hash0len
    ht_ksize   = 4 # hex digits for tree key
    hashtree   = [array("Q") for x in range(2**(ht_ksize*4))]
    chtree     = [array("I") for x in range(2**(ht_ksize*4))]
    chtree_max = 2**(chtree[0].itemsize*8) # "I" has 32bit range
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    ses_w = 2; ch_w = chdigits //2
    # limit number of sessions to ses_w + room for vol set:
    sessions   = aset.allsessions[:2**(ses_w*8)-(len(aset.vols))-1]
    addrsplit  = -address_split[1]

    dataf = open(tmpdir+"/hashindex.dat","w+b")
    dataf_write = dataf.write
    if listfile:
        dedupf = open(tmpdir+"/"+listfile, "w")

    count = match = 0
    for sesnum, ses in enumerate(sessions):
        volname = ses.volume.name; sesname = ses.name
        with open(pjoin(ses.path,"manifest"),"r") as manf:
            for ln in manf:
                ln1, ln2 = ln.split()
                if ln1 == "0":
                    continue
                bhashb = bytes().fromhex(ln1)
                #bhash = int(ln1[:hash0len*2], 16)
                i      = int(ln1[:ht_ksize], 16)

                ht = hashtree[i]; ct = chtree[i]
                while True:
                    try:
                        pos = ht.index(int.from_bytes(bhashb[:hash0len],
                                                      "little"))
                    except ValueError:
                        if count < chtree_max:
                            hashtree[i].frombytes(bhashb)
                            chtree[i].append(count)
                            dataf_write(sesnum.to_bytes(ses_w,"big"))
                            dataf_write(bytes().fromhex(ln2[1:]))
                            count += 1
                            break # while

                    if pos % hsegs == 0 and \
                       ht[pos+1:pos+hsegs].tobytes() == bhashb[hash0len:]:
                        #First hash segment matched; test remaining segments.
                        if listfile:
                            data_i = ct[pos//hsegs]
                            dataf.seek(data_i*(ses_w+ch_w))
                            ddses  = sessions[int.from_bytes(
                                     dataf.read(ses_w),"big")]
                            ddchx  = dataf.read(ch_w).hex().zfill(chdigits)
                            print("%s/%s/%s/x%s %s/%s/%s/%s" % \
                                (ddses.volume.name, ddses.name, ddchx[:addrsplit], ddchx,
                                volname, sesname, ln2[1:addrsplit], ln2),
                                file=dedupf)
                            dataf.seek(0,2)
                        match += 1
                        break # while

                    pos += hsegs - (pos % hsegs)
                    ht = ht[pos:]; ct = ct[pos//hsegs:]

    if listfile:
        dedupf.close()
        dataf.close()

    aset.hashindex = (sessions, hashtree, ht_ksize, hashdigits, hash_w, hash0len,
                      dataf, chtree, chdigits, ch_w, ses_w)


    if not options.debug:  return
    print("\n %d matches in %.1f seconds." % (match, int(time.time()-ctime)))
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\nMemory use: Max %dMB, index count: %d" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024,
         count)
        )
    print("Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


def init_dedup_index5(listfile=""):

    ctime = time.time()
    # Define arrays and element widths
    hashdigits = hash_bits // 4  # 256 @4bits per hex digit
    hash_w     = hash_bits // 8
    ht_ksize   = 4 # hex digits for tree key
    hashtree   = [bytearray() for x in range(2**(ht_ksize*4))]
    chtree     = [array("I") for x in range(2**(ht_ksize*4))]
    chtree_max = 2**(chtree[0].itemsize*8) # "I" has 32bit range
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    ses_w = 2; ch_w = chdigits //2
    # limit number of sessions to ses_w range:
    sessions   = aset.allsessions[:2**(ses_w*8)-(len(aset.vols))-1]
    addrsplit  = -address_split[1]

    dataf  = open(tmpdir+"/hashindex.dat","w+b")
    dataf_write = dataf.write
    if listfile:
        dedupf = gzip.open(tmpdir+"/"+listfile, "wt")

    count = match = 0
    for sesnum, ses in enumerate(sessions):
        volname = ses.volume.name; sesname = ses.name
        with open(pjoin(ses.path,"manifest"),"r") as manf:
            for ln in manf:
                ln1, ln2 = ln.split()
                if ln1 == "0":
                    continue
                bhashb = bytes().fromhex(ln1)
                i      = int(ln1[:ht_ksize], 16)
                pos    = hashtree[i].find(bhashb)
                if pos % hash_w == 0:
                    match += 1
                    if listfile:
                        data_i = chtree[i][pos//hash_w]
                        dataf.seek(data_i*(ses_w+ch_w))
                        ddses  = sessions[int.from_bytes(
                                 dataf.read(ses_w),"big")]
                        ddchx  = dataf.read(ch_w).hex().zfill(chdigits)
                        print("%s/%s/%s/x%s %s/%s/%s/%s" % \
                            (ddses.volume.name, ddses.name, ddchx[:addrsplit], ddchx,
                            volname, sesname, ln2[1:addrsplit], ln2),
                            file=dedupf)
                        dataf.seek(0,2)
                elif count < chtree_max:
                    hashtree[i].extend(bhashb)
                    chtree[i].append(count)
                    dataf_write(sesnum.to_bytes(ses_w,"big"))
                    dataf_write(bytes().fromhex(ln2[1:]))
                    count += 1

    if listfile:
        dedupf.close()
        dataf.close()

    aset.hashindex = (sessions, hashtree, ht_ksize, hashdigits, hash_w,
                      dataf, chtree, chdigits, ch_w, ses_w)


    if not options.debug:  return
    print("\nIndexed in %.1f seconds." % int(time.time()-ctime))
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\nMemory use: Max %dMB, index count: %d, matches: %d" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024,
         count, match)
        )
    print("Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))
    #print("idx size: %d" % sys.getsizeof(idx))


# Deduplicate data already in archive

def dedup_existing():

    print("Building deduplication index...")
    init_dedup_index("dedup.lst.gz")

    print("Linking...")
    do_exec([dest_run_args(desttype, [destcd + bkdir
               +" && /bin/cat >"+tmpdir+"-rpc/dest.lst.gz"
               +" && /usr/bin/python3 "+tmpdir+"-rpc/dest_helper.py dedup"
               ])
            ], infile=tmpdir+"/dedup.lst.gz")


# Controls flow of monitor and send_volume procedures:

def monitor_send(datavols, selected=[], monitor_only=True):
    for prg in (CP.lvm, CP.blkdiscard, CP.dmsetup, CP.thin_delta ):
        if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)

    localtime = time.strftime("%Y%m%d-%H%M%S")

    datavols, newvols \
        = prepare_snapshots(selected if len(selected) >0 else datavols)

    get_lvm_vgs(aset.vgname)
    if aset.vgname not in volgroups.keys():
        raise ValueError("Volume group "+aset.vgname+" not present.")

    if monitor_only:
        newvols = []
        volumes = []

    if len(datavols)+len(newvols) == 0:
        x_it(0, "No new data.")

    if len(datavols) > 0:
        get_lvm_deltas(datavols)

    if options.dedup:
        init_dedup_index()

    if compare_files(volumes=[x for x in aset.vols.values() if x.name in datavols]):
        x_it(1, "Error: Local and archive metadata differ.")

    if not monitor_only:
        print("\nSending backup session", localtime,
              "to", (desttype+"://"+destsys) if desttype != "internal" else destpath)

    for datavol in datavols+newvols:
        print("\nVolume :", datavol, flush=True)
        vol = aset.vols[datavol]

        map_updated \
                = update_delta_digest(datavol)

        if not monitor_only:
            if datavol in newvols or os.stat(vol.mapfile+"-tmp").st_blocks > 0:
                sent \
                    = send_volume(datavol, localtime)
                finalize_bk_session(vol, sent)
            else:
                print("  No changes.", flush=True)
                finalize_monitor_session(vol, False)
        else:
            finalize_monitor_session(vol, map_updated)


def init_deltamap(bmfile, bmsize):
    if exists(bmfile):
        os.remove(bmfile)
    if exists(bmfile+"-tmp"):
        os.remove(bmfile+"-tmp")
    with open(bmfile, "wb") as bmapf:
        bmapf.truncate(bmsize)    ; bmapf.flush()


def rotate_snapshots(vol, rotate=True):
    snap1vol = vol.name+".tick"
    snap2vol = vol.name+".tock"
    if rotate:
        #print("Rotating snapshots for", vol.name)
        # Review: this should be atomic
        lv_remove(aset.vgname, snap1vol)
        do_exec([[CP.lvm,"lvrename",aset.vgname+"/"+snap2vol, snap1vol]])
        l_vols[snap2vol].lv_name = l_vols[snap1vol].lv_name
        l_vols[snap1vol] = l_vols[snap2vol]
        del l_vols[snap2vol]

    else:
        lv_remove(aset.vgname, snap2vol)
        del l_vols[snap2vol]


def finalize_monitor_session(vol, map_updated):
    rotate_snapshots(vol, rotate=map_updated)
    os.rename(vol.mapfile+"-tmp", vol.mapfile)
    os.sync()


def finalize_bk_session(vol, sent):
    rotate_snapshots(vol, rotate=sent)
    init_deltamap(vol.mapfile, vol.mapsize())
    os.sync()


# Prune backup sessions from an archive. Basis is a non-overwriting dir tree
# merge starting with newest dirs and working backwards. Target of merge is
# timewise the next session dir after the pruned dirs.
# Specify data volume and one or two member list with start [end] date-time
# in YYYYMMDD-HHMMSS format.

def prune_sessions(datavol, times):

    # Validate date-time params
    for dt in times:
        datetime.datetime.strptime(dt, "%Y%m%d-%H%M%S")

    # t1 alone should be a specific session date-time,
    # t1 and t2 together are a date-time range.
    t1 = "S_"+times[0].strip()
    if len(times) > 1:
        if options.allbefore:  x_it(1, "Error: --all-before cannot be used with range.")
        t2 = "S_"+times[1].strip()
        if t2 <= t1:  x_it(1, "Error: Second date-time must be later than first.")
    else:
        t2 = ""

    print("\nPruning Volume :", datavol)

    volume = aset.vols[datavol]
    sessions = volume.sesnames
    if len(sessions) < 2:
        print("  No extra sessions to prune.")
        return

    # Find specific sessions to prune;
    # Use contiguous ranges.
    to_prune = []
    if options.allbefore:
        for ses in sessions:
            if ses >= t1:
                break
            to_prune.append(ses)

    elif t2 == "":
        if t1 in sessions:
            to_prune.append(t1)

    else:
        start = len(sessions)   ; end = 0
        if t1 in sessions:
            start = sessions.index(t1)
        else:
            for ses in sessions:
                if ses > t1:
                    start = sessions.index(ses)
                    break
        if t2 in sessions:
            end = sessions.index(t2)+1
        else:
            for ses in reversed(sessions):
                if ses < t2:
                    end = sessions.index(ses)+1
                    break
        to_prune = sessions[start:end]

    if len(to_prune) and to_prune[-1] == sessions[-1]:
        print("  Preserving latest session.")
        del to_prune[-1]
    if len(to_prune) == 0:
        print("  No selections in this date-time range.")
        return

    # Determine target session where data will be merged.
    target_s = sessions[sessions.index(to_prune[-1]) + 1]

    if not options.unattended and len(to_prune)>1:
        print("This will remove multiple sessions:\n"," ".join(to_prune))
        ans = input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    merge_sessions(datavol, to_prune, target_s,
                   clear_sources=True)


# Accepts a list of session names in ascending order (or else uses all sessions in the volume)
# and merges the manifests. Setting 'addcol' will add a colunm showing the session dir name.

def merge_manifests(datavol, msessions=None, mtarget=None, addcol=False):
    # Enh: implement mtarget to support merge_sessions()
    volume    = aset.vols[datavol]
    msessions = volume.sesnames if not msessions else msessions
    tmp = big_tmpdir if volume.volsize > 128000000000 else tmpdir
    outfile   = tmp+"/manifest.mrg"     ; slist  = []
    for suffix in ("/manifest\x00", "\x00"):
        with tempfile.NamedTemporaryFile(dir=tmp, delete=False) as tmpf:
            tmpf.write(bytes(suffix.join(reversed(msessions)), encoding="UTF-8"))
            tmpf.write(bytes(suffix, encoding="UTF-8"))
            slist.append(tmpf.name)

    if addcol:
        # add a column containing the source session
        cdir  = tmp+"/m"     ; slsort  = slist[1]
        shutil.rmtree(cdir, ignore_errors=True);   os.makedirs(cdir)

        # fix: extrapolate path with filename
        do_exec([[CP.xargs, "-0", "-a", slist[0],
                  CP.awk, '{sub("/manifest","",FILENAME); print $0, FILENAME > "'
                            +tmp+'/m/"FILENAME}']], cwd=volume.path)
    else:
        cdir  = volume.path     ; slsort  = slist[0]

    do_exec([[CP.sort, "-umd", "-k2,2", "--batch-size=64", "--files0-from="+slsort]],
            out=outfile, cwd=cdir)
    if addcol:  shutil.rmtree(tmp+"/m")

    return outfile


# Merge sessions together. Starting from first session results in a target
# that contains an updated, complete volume. Other starting points can
# form the basis for a pruning operation.
# Specify the data volume (datavol), source sessions (sources), and
# target. Caution: clear_sources is destructive.

def merge_sessions(datavol, sources, target, clear_sources=False):

    volume     = aset.vols[datavol]    ; resume = aset.in_process is not None
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    chformat   = "x%0"+str(chdigits)+"x"
    m_tmp      = tmpdir if volume.volsize < 128000000000 else big_tmpdir

    # Prepare manifests for efficient merge using fs mv/replace. The target is
    # included as a source, and oldest source is our target for mv. At the end
    # the merge_target will be renamed to the specified target. This avoids
    # processing the full range of volume chunks in the likely case that
    # the oldest (full) session is being pruned.
    merge_target  = sources[0]    ; merge_sources = ([target] + list(reversed(sources)))[:-1]
    os.chdir(volume.path)

    if not resume:
        for ses in sources + [target]:
            if volume.sessions[ses].format == "tar":
                x_it(1, "Cannot merge range containing tarfile session.")
        volsize    = volume.sessions[target].volsize
        vol_shrank = volsize < max([x.volsize for x in volume.sessions.values()
                                    if x.name in sources])
        if vol_shrank: print("Volume has shrunk.") ####
        last_chunk = chformat % volume.last_chunk_addr(volsize)
        lc_filter  = '"'+last_chunk+'"'

        with open("merge.tmp", "wt") as lstf:
            print(merge_target, target, file=lstf)

            # Get manifests, append session name to eol, print session names to list.
            #print("  Reading manifests")
            manifests = []
            for ses in merge_sources:
                if clear_sources:   print(ses, file=lstf)    ; manifests.append("man."+ses)
                do_exec([[CP.sed, "-E", "s|$| "+ses+"|", ses+"/manifest"
                        ]], out=m_tmp+"/man."+ses)
            print("###", file=lstf)

        # Unique-merge filenames: one for rename, one for new full manifest.
        do_exec([[CP.sort, "-umd", "-k2,2", "--batch-size=64"] + manifests],
                out="manifest.one", cwd=m_tmp)
        do_exec([[CP.sort, "-umd", "-k2,2", "manifest.one",
                pjoin(volume.path, merge_target, "manifest")]], out="manifest.two", cwd=m_tmp)
        # Make final manifest without extra column.
        do_exec([[CP.awk, "$2<="+lc_filter+" {print $1, $2}", m_tmp+"/manifest.two"]],
                out=target+"/manifest.tmp")

        # Output manifest filenames in the sftp-friendly form:
        # 'rename src_session/subdir/xaddress target/subdir/xaddress'
        # then pipe to destination and run dest_helper.py.
        do_exec([
                [CP.awk, "$2<="+lc_filter] if vol_shrank else None,
                [CP.sed, "-E",

                "s|^0 x(\S{" + str(address_split[0]) + "})(\S+)\s+(S_\S+)|"
                "-rm "+merge_target+"/\\1/x\\1\\2|; t; "

                "s|^\S+\s+x(\S{" + str(address_split[0]) + "})(\S+)\s+(S_\S+)|"
                "rename \\3/\\1/x\\1\\2 "+merge_target+"/\\1/x\\1\\2|"
                ]
                ], infile=m_tmp+"/manifest.one", out=">>merge.tmp")

        if vol_shrank:
            # If volume size shrank in this period then make trim list.
            do_exec([[CP.awk, "$2>"+lc_filter, m_tmp+"/manifest.two"],
                    [CP.sed, "-E", "s|^\S+\s+x(\S{" + str(address_split[0]) + "})(\S+)|"
                    "-rm "+merge_target+"/\\1/x\\1\\2|"]
                    ], out=">>merge.tmp")

        do_exec([[CP.gzip, "-f", "merge.tmp"]])

        # Update info records
        if clear_sources:
            for ses in sources:
                affected = volume.delete_session(ses, remove=False)
            volume.sessions[target].save_info("info.tmp")  ; volume.save_volinfo("volinfo.tmp")
            print("Removing %d" % len(sources), end="...", flush=True)

        # Set archive in_process state to "merge"
        set_in_process(["merge", datavol, str(clear_sources), target, sources],
                       tmp=True, dest=False)

        # Send new metadata and process lists to dest
        do_exec([[CP.tar, "-cf", "-", "../in_process.tmp", "volinfo.tmp", "merge.tmp.gz", target],
                dest_run_args(desttype, [destcd + bkdir+"/"+datavol
                +" && tar -xmf -"
                +" && mv merge.tmp.gz merge.lst.gz"
                +" && mv in_process.tmp ../in_process"
                ])
            ])
        os.replace("../in_process.tmp", "../in_process")

    retcode = dest_run([destcd + bkdir+"/"+datavol +" && python3 "
                        +tmpdir+"-rpc/dest_helper.py merge"+(" --resume" if resume else "")],
                        check=False)
    if retcode > 0:  print("Merge failed code", retcode)

    if retcode == 50:
        print("Aborting merge initialization.")   ; set_in_process(None)
        for f in ("merge.tmp.gz","volinfo.tmp",target+"/info.tmp"):
            if exists(f):  os.remove(f)
        dest_run([destcd + bkdir+"/"+datavol
                  +" && rm -f merge.tmp.gz merge.lst.gz volinfo.tmp"], check=False)
    elif retcode >0:
        x_it(retcode, "Exiting.")
    else:
        if exists("volinfo.tmp"):  os.replace("volinfo.tmp", "volinfo")
        for f in ("/info","/manifest"):
            if exists(target+f+".tmp"):  os.replace(target+f+".tmp", target+f)
        set_in_process(None, dest=False)   ; os.remove("merge.tmp.gz")
        for ses in sources:  shutil.rmtree(volume.path+"/"+ses, ignore_errors=True)
    do_exec([[CP.sync, "-f", "volinfo"]])
    if resume and compare_files(volumes=[volume], sessions=[volume.sessions[target]]):
        x_it(1, "Error: Local and dest metadata differ.")

    print("done.")


# Compare files between local and dest archive, using hashes.
# The file tmpdir/compare-files.lst can be pre-populated with file paths if clear=False;
# otherwise will build metadata file list from Volume & Ses objects.
# Returns False if local and dest hashes match.

def compare_files(pathlist=[], volumes=[], sessions=[], clear=True):
    cmp_list = tmpdir+"/compare-files.lst"
    if clear and exists(cmp_list):  os.remove(cmp_list)
    realvols  = [x for x in volumes if len(x.sessions)]
    realpaths = [x for x in pathlist if exists(x)]
    if len(realvols) or len(sessions) or len(realpaths):
        flist = open(cmp_list, "a")
    else:
        #print("Compare list is empty.")
        return False
    for v in realvols:  print(v.name+"/volinfo", file=flist)
    for s in sessions:
        for sf in ("info","manifest"): print(pjoin(s.volume.name,s.name,sf), file=flist)
    for pth in realpaths:  print(pth, file=flist)
    flist.close()
    do_exec([[CP.xargs, CP.sha256sum]], cwd=aset.path,
            infile=cmp_list, out=tmpdir+"/compare-hashes.local")
    dest_run([destcd + bkdir +" && xargs sha256sum"],
             infile=cmp_list, out=tmpdir+"/compare-hashes.dest")
    # maybe switch cmp to diff/sha256sum if they are safe enough to read untrusted input
    return do_exec([[CP.cmp, tmpdir+"/compare-hashes.local",tmpdir+"/compare-hashes.dest"]],
                check=False) > 0


# Receive volume from archive. If no save_path specified, then verify only.
# If diff specified, compare with current local volume; with --remap option
# can be used to resync volume with archive if the deltamap or snapshots
# are lost or if the local volume reverted to an earlier state.

def receive_volume(datavol, select_ses="", save_path="", diff=False):

    def diff_compare(dbuf,z):
        if dbuf != cmpf.read(chunksize):
            if attended:  print(" delta", faddr, "Z   " if z else "    ", end="\x0d")
            if remap:
                volsegment = addr // chunksize 
                bmap_pos = volsegment // 8
                bmap_mm[bmap_pos] |= 1 << (volsegment % 8)
            return len(dbuf)
        else:
            return 0

    verify_only = options.action == "verify"    ; remap  = options.remap
    attended    = not options.unattended        ; debug  = options.debug
    gethash     = hash_funcs[aset.hashtype] 
    decompress  = compressors[aset.compression][0].decompress

    vgname    = aset.vgname
    vol       = aset.vols[datavol]
    chunksize = aset.chunksize
    zeros     = bytes(chunksize)
    snap1vol  = datavol+".tick"
    sessions  = vol.sesnames

    if diff or verify_only:
        save_path = ""
    elif not save_path:
        save_path = pjoin("/dev",vgname,datavol)

    if save_path or diff:
        for prg in (CP.lvm, CP.blkdiscard):
            if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)

    # Set the session to retrieve
    if select_ses:
        datetime.datetime.strptime(select_ses, "%Y%m%d-%H%M%S")
        select_ses = "S_"+select_ses
        if select_ses not in sessions:
            x_it(1, "The specified session date-time does not exist.")
    elif len(sessions) > 0:
        select_ses = sessions[-1]
    else:
        x_it(1, "No sessions available.")

    volsize   = vol.sessions[select_ses].volsize

    if save_path and exists(save_path) and attended:
        print("\n!! This will erase all existing data in",save_path,"!!")
        ans = input("   Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    chdigits    = max_address.bit_length() // 4 # 4bits per digit
    chformat    = "x%0"+str(chdigits)+"x"
    lchunk_addr = vol.last_chunk_addr(volsize)
    last_chunkx = chformat % lchunk_addr

    # Collect session manifests
    include = False   ; incl_ses = []
    for ses in sessions:
        incl_ses.append(vol.sessions[ses])

        if vol.sessions[ses].format == "tar":
            raise NotImplementedError("Receive from tarfile not yet implemented: "+ses)

        if ses == select_ses:  break

    # compare metadata hashes
    if not options.from_arch:
        #print("Checking metadata..")
        if compare_files(volumes=[vol], sessions=incl_ses):
            x_it(1, "Error: Local and archive metadata differ.")

    # Merge manifests and send to archive system:
    # sed is used to expand chunk info into a path and filter out any entries
    # beyond the current last chunk, then piped to cat on destination.
    # Note address_split is used to bisect filename to construct the subdir.
    manifest = merge_manifests(datavol, msessions=[x.name for x in incl_ses], addcol=True)

    cmds = [[CP.sed, "-E", "/"+last_chunkx+"/q", manifest],  #### Enh: detect vol_shrank
            [CP.sed, "-E", "/^0\s/ d; "
             "s|^\S+\s+x(\S{" +str(address_split[0])+ "})(\S+)\s+(S_\S+)|\\3/\\1/x\\1\\2|;"],
            [CP.gzip, "-c", "-4"
            ],
            dest_run_args(desttype, ["cat >"+tmpdir+"-rpc/dest.lst.gz"]),
           ]
    do_exec(cmds)

    # Prepare save volume
    if save_path:
        # Decode dev path semantics and match to vg/lv if possible. Otherwise, open
        # simple block dev or file.
        save_type = "block device"
        returned_home = False    ; lv, pool, vg = get_lv_path_pool(save_path)
        if not lv and vg_exists(os.path.dirname(save_path)):
            # Got vg path, lv does not exist
            lv = os.path.basename(save_path)
            vg = os.path.basename(os.path.dirname(save_path))
        if vg:
            # Does save path == original path?
            returned_home = (lv== datavol) and (vg== aset.vgname) and not options.from_arch
            save_type = "logical volume"
            if not lv_exists(vg,lv):
                if vg != vgname:
                    x_it(1, "Cannot auto-create volume: Volume group does not match config.")
                print("Creating '%s' in thin pool [%s/%s]." % (lv, vg, aset.poolname))
                do_exec([[CP.lvm, "lvcreate", "-kn", "-ay", "-V", str(volsize)+"b",
                          "--thin", "-n", lv, vg+"/"+aset.poolname]])
            elif l_vols[lv].lv_size != volsize:
                print("Re-sizing LV to %d bytes." % volsize)
                do_exec([[CP.lvm, "lvresize", "-L", str(volsize)+"b", "-f", save_path]])

        if exists(save_path) and stat.S_ISBLK(os.stat(save_path).st_mode):
            do_exec([[CP.blkdiscard, save_path]])
            savef = open(save_path, "w+b")
        elif save_path.startswith("/dev/"):
            x_it(1,"Cannot create new volume from ambiguous /dev path."
                 " Please create the volume before using 'receive', or specify"
                 " --save-to=/dev/volgroup/lv in case of a thin LV.")
        else: # file
            save_type = "file"
            savef = open(save_path, "w+b")
            savef.truncate(0)          ; savef.flush()
            savef.truncate(volsize)    ; savef.flush()

    elif diff:
        if not lv_exists(vgname, datavol):
            x_it(1, "Local volume must exist for diff.")
        if remap:
            if not lv_exists(vgname, snap1vol):
                do_exec([[CP.lvm,"lvcreate", "-pr", "-kn", "-ay",
                        "-s", vgname+"/"+datavol, "-n", snap1vol]])
                print("  Initial snapshot created for", datavol)
                get_lvm_vgs(aset.vgname)
            if not exists(vol.mapfile):
                init_deltamap(vol.mapfile, vol.mapsize())
            bmapf = open(vol.mapfile, "r+b")
            bmapf.truncate(vol.mapsize())    ; bmapf.flush()
            bmap_mm = mmap.mmap(bmapf.fileno(), 0)
        else:
            if not lv_exists(vgname, snap1vol):
                print("Snapshot '.tick' not available; Comparing with source volume instead.")
                snap1vol = datavol

            if volsize != l_vols[snap1vol].lv_size:
                x_it(1, "Volume sizes differ:"
                    "\n  Archive = %d \n  Local   = %d" % (volsize, l_vols[snap1vol].lv_size))

        cmpf  = open(pjoin("/dev",vgname,snap1vol), "rb")
        diff_count = 0

    print("Receiving" if save_path else "Scanning", "volume :", datavol, select_ses)
    if save_path:    print("Saving to %s '%s'" % (save_type, save_path))
    # Create retriever process using py program
    cmd = dest_run_args(desttype,
            [destcd + bkdir+"/"+datavol
            +"  && python3 "+tmpdir+"-rpc/dest_helper.py receive"
            ])
    getvol = subprocess.Popen(cmd, stdout=subprocess.PIPE)

    # Open manifest then receive, check and save data
    bcount = 0
    with open(manifest, "r") as mf:
        for addr in range(0, volsize, chunksize):
            faddr = chformat % addr
            if attended:
                print(int(addr/volsize*100),"%  ",faddr,end="  ")

            cksum, fname, ses = mf.readline().split()
            if fname != faddr:
                raise ValueError("Address mismatch "+fname+" expected "+faddr)

            if cksum.strip() == "0":
                if attended:  print("OK",end="\x0d")
                if save_path:
                    savef.seek(chunksize, 1)
                elif diff:
                    diff_count += diff_compare(zeros,True)

                continue

            # Read chunk size
            untrusted_size = int.from_bytes(getvol.stdout.read(4),"big")

            # allow for slight expansion from compression algo
            if untrusted_size > chunksize + (chunksize // 1024) \
                or untrusted_size < 1:
                    raise BufferError("Bad chunk size: %d" % untrusted_size)

            # Size is OK.
            size = untrusted_size

            # Read chunk buffer
            untrusted_buf = getvol.stdout.read(size)
            rc  = getvol.poll()
            if rc is not None and len(untrusted_buf) == 0:
                break

            if len(untrusted_buf) != size:
                with open(tmpdir+"/bufdump", "wb") as dump:
                    dump.write(untrusted_buf)
                raise BufferError("Got %d bytes, expected %d"
                                  % (len(untrusted_buf), size))
            if cksum != gethash(untrusted_buf).hexdigest():
                with open(tmpdir+"/bufdump", "wb") as dump:
                    dump.write(untrusted_buf)
                raise ValueError("Bad hash "+fname+" :: "+gethash(untrusted_buf).hexdigest())

            if verify_only:
                if attended:  print("OK",end="\x0d")
                continue

            # Proceed with decompress.
            untrusted_decomp = decompress(untrusted_buf)
            if len(untrusted_decomp) != chunksize and addr < lchunk_addr:
                raise BufferError("Decompressed to %d bytes." % len(untrusted_decomp))
            if addr == lchunk_addr and len(untrusted_decomp) != volsize - lchunk_addr:
                raise BufferError("Decompressed to %d bytes." % len(untrusted_decomp))

            # Buffer is OK...
            buf = untrusted_decomp    ; bcount += len(buf)
            if attended:
                print("OK",end="\x0d")

            if save_path:
                savef.write(buf)
            elif diff:
                diff_count += diff_compare(buf,False)

        if rc is not None and rc > 0:
            raise RuntimeError("Error code from getvol process: "+str(rc))

        buf = decompress(untrusted_buf)
        if addr+len(buf) != volsize:
            raise ValueError("Received range %d does not match volume size %d."
                             % (addr+len(buf), volsize))
        print("100%")
        if debug:
            print("Received range:", addr+len(buf),
                "\n    Data bytes:", bcount)

        if save_path:
            savef.flush() ; savef.close()
            if returned_home:
                if not lv_exists(vgname, snap1vol):
                    do_exec([[CP.lvm, "lvcreate", "-pr", "-kn",
                        "-ay", "-s", vgname+"/"+datavol, "-n", snap1vol]])
                    print("  Initial snapshot created for", datavol)
                if not exists(vol.mapfile):
                    init_deltamap(vol.mapfile, vol.mapsize())
                if select_ses != sessions[-1]:
                    print("Restored from older session: Volume may be out of"
                        " sync with archive until '%s --remap diff %s' is run!"
                        % (prog_name, datavol))
        elif diff:
            cmpf.close()
            if remap:
                bmapf.close()
                print("Delta bytes re-mapped:", diff_count)
                if diff_count > 0 and options.action != "send":
                    print("\nNext 'send' will bring this volume into sync.")
            elif diff_count:
                x_it(1, "%d bytes differ." % diff_count)


# Rename a volume in the archive

def rename_volume(archive, oldname, newname):
    for prg in (CP.lvm, CP.blkdiscard):
        if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)

    aset = ArchiveSet(archive.name, metadir+topdir, allvols=True)
    if oldname not in aset.vols:  x_it(1,"No '%s' found." % oldname)
    v = aset.vols[oldname]     ; meta_only = not exists(v.path) or not len(v.sessions)
    if not meta_only and compare_files(volumes=[v]):
        x_it(1, "Error: Local and archive metadata differ.")

    aset.add_volume(newname)   ; aset.conf["volumes"][newname] = aset.conf["volumes"][oldname]
    confname = os.path.basename(aset.confpath)
    del aset.conf["volumes"][oldname]   ; aset.save_conf(confname+".tmp")

    if not aset.in_process:
        set_in_process(["rename", oldname, newname], dest=not meta_only)
    if exists(v.path):
        os.replace(v.path, aset.path+"/"+newname)
    if not meta_only:
        dest_run([destcd + bkdir
                +"  && cat >"+confname
                +(" && { if [ -e '%s' ]; then  rm -rf '%s'" % (oldname, newname))
                +(" && mv -T '%s' '%s'" % (oldname, newname)) + "; fi }"
                +"  && sync -f ."],  infile=aset.path+"/"+confname+".tmp")
    os.replace(aset.path+"/"+confname+".tmp", aset.confpath)
    was_in_process = aset.in_process is not None
    set_in_process(None)

    # cleanup
    if lv_exists(aset.vgname, oldname+".tick"):
        do_exec([[CP.lvm,"lvrename",aset.vgname+"/"+oldname+".tick", newname+".tick"]])
    if lv_exists(aset.vgname, oldname+".tock"):  lv_remove(aset.vgname, oldname+".tock")
    get_lvm_vgs(aset.vgname)
    if was_in_process and not meta_only and \
        compare_files(pathlist=["archive.ini"], volumes=[aset.vols[newname]]):
            x_it(1, "Error: Local and archive metadata differ.")
        # Enh: que automatic remap

    return ArchiveSet(archive.name, metadir+topdir)


# Remove a volume from the archive

def delete_volume(dv):
    for prg in (CP.lvm, CP.blkdiscard):
        if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)

    if not options.unattended and not aset.in_process:
        print("Warning! Delete will remove ALL metadata AND archived data",
              "for volume", dv)

        ans = input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    print("\nDeleting volume", dv, "from archive.")
    if not aset.in_process:       set_in_process(["delete", dv])
    if dv in aset.vols:           aset.delete_volume(dv)
    if exists(aset.path+"/"+dv):  shutil.rmtree(aset.path+"/"+dv)

    confname = os.path.basename(aset.confpath)
    cmd = [destcd + bkdir
          +"  && cat >"+confname
          +(" && rm -rf '%s'" % dv)
          +"  && sync -f ."
          ]
    dest_run(cmd, infile=aset.confpath)
    set_in_process(None)
    return


def show_list(selected_vols):
    # Print list of volumes if no volume is selected.
    if not selected_vols and not len(options.volumes):
        print("\nConfigured Volumes [%s/%s]\n" % (aset.vgname, aset.poolname))
        maxwidth = max([len(x.name) for x in aset.vols.values()])
        fmt    = "%7.1f GB  %-" + str(maxwidth+5) + "s  %s"
        for name in sorted([x.name for x in aset.vols.values()]):
            vol = aset.vols[name]   ; ses = vol.sesnames[-1][2:] if len(vol.sesnames) else " "
            print(fmt % ((aset.vols[name].volsize / 1024**3), name, ses))
        return

    # Print list of sessions for selected volume(s).
    # Get the terminal column width and set cols to number of list columns.
    ttycols = os.popen('stty size', 'r').read().split()[1]
    cols = max(4, min(10, int(ttycols)//17))
    for dv in selected_vols:
        print("\nSessions for volume '%s':\n" % dv)
        vol = aset.vols[dv]    ; lmonth = vol.sesnames[0][2:8]    ; slist = []

        # Blank at end of 'sesnames' is a terminator that doesn't match any month value.
        for ses in vol.sesnames + [""]:
            month = ses[2:8]
            if options.unattended:
                # plain listing
                print(ses)
                continue

            if month == lmonth:
                # group sessions by month
                slist.append(ses)
            else:
                # print the month listing: 'rows' is adjusted to carry the remainder on
                # additional lines. 'extra' and 'steps' are used to eliminate ragged
                # column on the right (a ragged row is more pleasing to the eye).
                size  = len(slist)    ; rows = size//cols
                rows += (size%rows)//cols if rows else 0     ; extra = size - cols*rows
                heights = [rows+(x<extra) for x in range(cols) ]

                # output one row at a time
                for ii in range(max(heights)):
                    print("  ".join([  slist[ii+sum(heights[:c])][2:] for c in range(cols)
                                       if ii < heights[c]  ]))
                print()

                # start new month list
                lmonth = month    ; slist = [ses]


def show_mem_stats():
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\n  Memory use: Max %dMB" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024)
        )
    print("  Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


# Exit with simple message

def x_it(code, text):
    sys.stderr.write(text+"\n")
    cleanup()
    sys.exit(code)


def cleanup():
    shutil.rmtree(big_tmpdir, ignore_errors=True)   ; return ####
    if not options.debug:
        for f in (big_tmpdir, tmpdir, tmpdir+"-rpc"):
            shutil.rmtree(f, ignore_errors=True)




##  MAIN  #####################################################################################

# Constants / Globals
prog_name             = "wyng"
prog_version          = "0.2.0_beta5"
format_version        = 1

# Disk block size:
bs                    = 512
# LVM min blocks = 128 = 64kBytes:
lvm_block_factor      = 128
# Default archive chunk size = 64kBytes:
bkchunksize           = 1 * lvm_block_factor * bs
assert bkchunksize % (lvm_block_factor * bs) == 0
max_address           = 0xffffffffffffffff # 64bits
# for 64bits, a subdir split of 9+7 allows =< 4096 files per dir:
address_split         = [len(hex(max_address))-2-7, 7]
hash_bits             = 256

pjoin                 = os.path.join      ; exists =  os.path.exists

os.environ["LC_ALL"]  = "C"
shell_prefix          = "set -e && export LC_ALL=C\n"

url_types             = ("ssh://", "qubes-ssh://", "qubes://", "internal:")

ssh_opts              = ["-x", "-o", "ControlPath=~/.ssh/controlsocket-%r@%h-%p",
                         "-o", "ControlMaster=auto", "-o", "ControlPersist=60"]

local_actions         = ("index-test","monitor","list","version","add")


if sys.hexversion < 0x3050000:
    x_it(1, "Python ver. 3.5 or greater required.")

# Root user required
if os.getuid() > 0:
    x_it(1, "Must be root user.")

# Allow only one instance at a time
lockpath = "/var/lock/"+prog_name
try:
    lockf = open(lockpath, "w")
    fcntl.lockf(lockf, fcntl.LOCK_EX|fcntl.LOCK_NB)
except IOError:
    x_it(1, "ERROR: "+prog_name+" is already running.")

# Parse Arguments:
parser = argparse.ArgumentParser(description="")
parser.add_argument("action", choices=["send","monitor","add","delete","prune","receive",
                    "verify","diff","list","version","rename","arch-init","arch-delete",
                    "arch-deduplicate","index-test","test"],
                    default="monitor", help="Action to take")
parser.add_argument("-u", "--unattended", action="store_true", default=False,
                    help="Non-interactive, supress prompts")
parser.add_argument("-a", "--all", action="store_true", default=False,
                    help="Apply action to all volumes")
parser.add_argument("--all-before", dest="allbefore", action="store_true", default=False,
                    help="Select all sessions before --session date-time.")
#parser.add_argument("--tarfile", action="store_true", dest="tarfile", default=False,
#                    help="Store backup session as a tarfile")
parser.add_argument("--session", help="YYYYMMDD-HHMMSS[,YYYYMMDD-HHMMSS]"
                                 " select session date(s), singular or range.")
parser.add_argument("--from", dest="from_arch", default="",
                    help="Address+Path of other non-configured archive (receive, verify)")
parser.add_argument("--save-to", dest="saveto", default="",help="Path to store volume for receive")
parser.add_argument("--remap", action="store_true", default=False, help="Remap volume")
parser.add_argument("--local", default="",    help="Init: LVM vg/pool containing source volumes")
parser.add_argument("--dest", default="",     help="Init: type:location of archive")
parser.add_argument("--subdir", default="",   help="Init: optional subdir for --dest")
parser.add_argument("--compression", default="", help="Init: compression type:level")
parser.add_argument("--hashtype", default="", help="Init: hash function type")
parser.add_argument("--chunk-factor", dest="chfactor", type=int,
                    help="Init: set chunk size to N*64kB")
#parser.add_argument("--deltas", action="store_true", default=False,
#                    help="Receive: differential mode")
parser.add_argument("--meta-dir", dest="metadir", default="", help="Use alternate metadata path")
parser.add_argument("--debug", action="store_true", default=False, help="Debug mode")
parser.add_argument("--testing-dedup", dest="dedup", type=int, default=1,
                    help="Test experimental deduplication (send)")
parser.add_argument("volumes", nargs="*")
options = parser.parse_args()
#subparser = parser.add_subparsers(help="sub-command help")
#prs_prune = subparser.add_parser("prune",help="prune help")



# Setup tmp and metadata dirs
metadir     = options.metadir if options.metadir else "/var/lib"
topdir      = "/"+prog_name+".backup"
tmpdir      = "/tmp/"+prog_name
big_tmpdir  = metadir+topdir+"/.tmp"

shutil.rmtree(tmpdir+"-old", ignore_errors=True)
shutil.rmtree(big_tmpdir, ignore_errors=True)
if exists(tmpdir):
    os.rename(tmpdir, tmpdir+"-old")
os.makedirs(tmpdir+"/rpc")
os.makedirs(metadir+topdir+"/wyng.old", exist_ok=True)
os.makedirs(big_tmpdir, exist_ok=True)


## General Configuration ##

compressors      = {"zlib": (zlib, 4), "bz2": (bz2, 9)}

hash_funcs       = {"sha256": hashlib.sha256}
if "blake2b" in hashlib.algorithms_available:
    hash_funcs["blake2b"] = functools.partial(hashlib.blake2b, digest_size=hash_bits//8)

# Check --from usage (access other/unconfigured archive).
if options.from_arch and options.action not in ("receive","verify","list","arch-init"):
    x_it(1,"--from option can be used only with: receive, verify, list, arch-init")
# Select dedup test algorithm.
init_dedup_index = [None, init_dedup_index1, None, init_dedup_index3,
                    init_dedup_index4, init_dedup_index5][options.dedup]
monitor_only     = options.action == "monitor" # gather metadata without backing up

# Get LVM listings
volgroups, l_vols= {}, {}    ; get_lvm_vgs()

# Create an ArchiveSet object with get_configs() and set global vars pertaining
# to the destination.
destsys          = desttype  = dest_run_map  = dest_online  = dest_in_process  = None
aset             = get_configs()
destsys, desttype= detect_internal_state()
dest_online, dest_in_process  = detect_dest_state(destsys)
bkdir            = topdir+"/"+aset.name
destpath         = os.path.normpath(pjoin(aset.destmountpoint,aset.destdir))
destcd           = " cd '"+destpath+"'"

# Check online status for certain commands.
if not dest_online and (options.remap or options.from_arch \
                    or options.action not in local_actions): ## and destsys is not None:
    x_it(1, "Destination not ready to receive commands.")

if options.from_arch:
    aset         = get_configs_remote()
os.makedirs(metadir+bkdir, exist_ok=True)

# Vol group is now known: Set 'l_vols' to reference it.
if aset.vgname in volgroups.keys():
    l_vols       = volgroups[aset.vgname].lvs


# Handle unfinished in_process:
# Functions supported here must not internally use global variable inputs that are unique to
# a runtime invocation (i.e. the 'options' objects), or they must test such variables
# in conjunction with aset.in_process.
if aset.in_process and dest_online and not options.from_arch \
    and options.action != "arch-init":
    if exists(aset.path+"/in_process_retry"):
        x_it(1, "Interrupted process already retried; Exiting.")
    open(aset.path+"/in_process_retry","w").close()

    if options.action == "delete" and aset.in_process[1] == options.volumes[0]:
        # user is currently deleting the in_process volume
        set_in_process(None)
    elif aset.in_process[0] in ("delete","merge","rename"):
        print("Completing prior operation in progress:", " ".join(aset.in_process[0:2]))

        if aset.in_process[0] == "delete":
            delete_volume(aset.in_process[1])
        elif aset.in_process[0] == "rename":
            aset = rename_volume(aset, aset.in_process[1], aset.in_process[2])
        elif aset.in_process[0] == "merge":
            merge_sessions(aset.in_process[1], aset.in_process[4],
                        aset.in_process[3], clear_sources=bool(aset.in_process[2]))

    else:
        print("Unknown prior operation in progress:", aset.in_process[0:2])
        x_it(1,"Exiting.")
elif dest_in_process:
    x_it(1, "Archive locked in_process without local indicator. Exiting.")


# Check volume args against config
datavols      = aset.vols.keys()
selected_vols = options.volumes[:]
for vol in options.volumes:
    if vol not in datavols and options.action not in {"add","delete","rename"}:
        print("Volume "+vol+" not configured; Skipping.")
        del(selected_vols[selected_vols.index(vol)])


# Process Commands:

if options.action   == "monitor":
    monitor_send(datavols, monitor_only=True)


elif options.action == "send":
    monitor_send(datavols, selected_vols, monitor_only=False)


elif options.action == "version":
    print(prog_name, "version", prog_version)


elif options.action == "prune":
    if not options.session:
        x_it(1, "Must specify --session for prune.")
    dvs = datavols if len(selected_vols) == 0 else selected_vols
    for dv in dvs:
        if dv in datavols:
            prune_sessions(dv, options.session.split(","))


elif options.action == "receive":
    if len(selected_vols) != 1:
        x_it(1, "Specify one volume for receive.")
    if options.session and len(options.session.split(",")) > 1:
        x_it(1, "Specify only one session for receive.")
    receive_volume(selected_vols[0], select_ses="" if not options.session else options.session,
                   save_path=options.saveto if options.saveto else "")


elif options.action == "verify":
    if len(selected_vols) != 1:
        x_it(1, "Specify one volume for verify.")
    if options.session and len(options.session.split(",")) > 1:
        x_it(1, "Specify one session for verify.")
    receive_volume(selected_vols[0],
                   select_ses="" if not options.session \
                   else options.session.split(",")[0],
                   save_path="")


elif options.action == "diff":
    if selected_vols:
        receive_volume(selected_vols[0], save_path="", diff=True)


elif options.action == "list":
    show_list(selected_vols)


elif options.action == "add":
    if len(options.volumes) < 1:
        x_it(1, "A volume name is required for 'add' command.")

    aset.add_volume(options.volumes[0])
    print("Volume", options.volumes[0], "added to archive config.")


elif options.action == "rename":
    if len(options.volumes) != 2:  x_it(1,"Rename requires two volume names.")
    rename_volume(aset, options.volumes[0], options.volumes[1])


elif options.action == "delete":
    delete_volume(selected_vols[0])


elif options.action == "untar":
    raise NotImplementedError()


elif options.action == "arch-init":
    # handled by get_configs, get_configs_remote
    pass


elif options.action == "arch-delete":
    print("Warning! Wipe-all will remove ALL metadata AND archived data!")

    ans = input("Are you sure? [y/N]: ")
    if ans.lower() not in {"y","yes"}:
        x_it(0,"")

    for dv in list(aset.vols):
        aset.delete_volume(dv)

    shutil.rmtree(aset.path, ignore_errors=True)

    print("\nDeleting entire archive...")
    cmd = [destcd
          +" && rm -rf ."+bkdir
          +" && sync -f ."
          ]
    dest_run(cmd)


elif options.action == "arch-deduplicate":
    if options.dedup > 1:
        dedup_existing()
    else:
        x_it(1,"Requires '--testing-dedup value greater than 1.")


if options.action   == "index-test":
    init_dedup_index()
elif options.action == "test":
    pass
    #merge_manifests("root", addcol=True)
    #merge_sessions("root", aset.vols["root"].sesnames[:-1],  aset.vols["root"].sesnames[-1], clear_sources=True)


cleanup()
